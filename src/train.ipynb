{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b50b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from typing import Tuple, Optional\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0f0cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import(\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class TrainConfig:\n",
    "    features_path: str = \"\"\n",
    "    lables_path: str = \"\"\n",
    "    output_dir: str = \"artifacts\"\n",
    "    model_type: str = \"rf\"\n",
    "    test_size: float = 0.2\n",
    "    random_state: int = 42\n",
    "    max_iter: int = 1000\n",
    "    n_estimator: int = 300\n",
    "    n_jobs: int = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5824377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features_and_label(features_path, labels_path):\n",
    "    # Read with index_col=0 to drop the first unnamed index column if present\n",
    "    features_df = pd.read_csv(features_path, index_col = 0)\n",
    "    labels_df = pd.read_csv(labels_path, index_col=0)\n",
    "\n",
    "    # Align by index to ensure consistent row ordering and length\n",
    "    missing_in_features = labels.index.difference(features_df.index)\n",
    "    if len(missing_in_features) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Labels contain indices not present in features: {len(missing_in_features)} missing\"\n",
    "        )\n",
    "    \n",
    "    labels = labels_df[\"Exited\"].astype(int)\n",
    "    \n",
    "    # align features rows by labels indexes\n",
    "    aligned_features_df = features_df.loc[labels.index]\n",
    "\n",
    "    return aligned_features_df, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3a30f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pipeline(config: TrainConfig):\n",
    "    if config.model_type == \"logreg\":\n",
    "        model = LogisticRegression(\n",
    "            max_iter = config.max_iter,\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=config.random_state,\n",
    "            n_jobs=config.n_jobs\n",
    "        )\n",
    "        steps = [(\"scaler\", StandardScaler()), (\"model\", model)]\n",
    "        return Pipeline(steps=steps)\n",
    "    elif config.model_type == \"rf\":\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=config.n_estimator,\n",
    "            random_state=config.random_state,\n",
    "            n_jobs=config.n_jobs,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "        )\n",
    "        return Pipeline(steps=[(\"model\", model)])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError (\"Unsuported model_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "801c1c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_prob):\n",
    "    metrics: dict = {}\n",
    "    metrics[\"accuracy\"] = float(accuracy_score(y_true, y_pred))\n",
    "    metrics[\"precision\"] = float(precision_score(y_true, y_pred))\n",
    "    metrics[\"recall\"] = float(recall_score(y_true, y_pred))\n",
    "    metrics[\"F1_score\"] = float(f1_score(y_true, y_pred))\n",
    "    metrics[\"ROC-AUC_score\"] = float(roc_auc_score(y_true, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics[\"confusion_matrix\"] = cm.tolist()\n",
    "    metrics[\"classification_report\"] = classification_report(y_true, y_pred, digits=4)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9482be8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
